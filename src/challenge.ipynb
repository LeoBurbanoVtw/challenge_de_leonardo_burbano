{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autor:** Leonardo Burbano\n",
    "<br/>\n",
    "**Empresa:** TW\n",
    "<br/>\n",
    "**Email:** leonardo.burbano@[dominioTW]\n",
    "<br/>\n",
    "**Mes/Año:** 05/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Suposiciones\n",
    "\n",
    "- La descarga/carga del archivo de input no se realizará de forma recurrente. Se realizará una sola vez para efectos de solucionar el presente desafío.\n",
    "- La ejecución de la descarga/descompresión/carga del archivo se realizará bajo demanda (y manualmente de ser el caso). Se considerará como variable el nombre del archivo, y como ubicación fija un bucket (repositorio) en Cloud Storage (GCP - Google Cloud Platform).\n",
    "- La prioridad es obtener la respuesta a las preguntas planteadas. No surgirán otras preguntas en el corto plazo.\n",
    "- Al final de este documento se detallan las recomendaciones para garantizar la escalabilidad del proceso y también la calendarización del mismo de ser necesario.\n",
    "- La respuesta proviene de una consulta a la API de Twitter que está almacenada en un archivo comprimido en Google Drive.\n",
    "- El desarrollo no incluye versionamiento de la infraestructura creada o utilización de infraestructura como código.\n",
    "- No se incluyen configuraciones de permisos y otras tareas de disposición de la infraestructura en GCP.\n",
    "\n",
    "#### 2. Configuración del ambiente\n",
    "- Se utilizó Git y Gitflow para Mac, para facilitar el versionamiento y simulación de trabajo colaborativo.\n",
    "- Se realizó una exploración inicial en la que se fue\n",
    "- Se utilizará la nube de GCP, se requieré únicamente un archivo de cuenta de servicio con los permisos adecuados para ejecutar el proyecto.\n",
    "\n",
    "\n",
    "#### 2. Estrategia y features a implementar\n",
    "\n",
    "##### 2.1. **feature/initial_exploration:** \n",
    "Se comienza tratando de llegar a la respuesta a las preguntas lo más rápido posible utilizando un Notebook de exploración y las configuraciones manuales en GCP (ejemplo: Cargar un archivo a Cloud Storage, cargar el archivo a BigQuery). Este tipo de pruebas dieron mayor visibilidad del esquema del archivo y los posibles desafíos a enfrentar. El detalle se encuentra en ../_initial_exploration/00_test.ipynb . A partir de esa exploración se realizó esta estrategia y varias de las decisiones tomadas a continuación.\n",
    "\n",
    "##### 2.2 **feature/environment_setup:** \n",
    "Se realiza la configuración del ambiente en GCP creando una \"Service Account\" en la sección de \"IAM\" con los permisos necesarios para cargar datos en BigQuery, Cloud Storage y otros servicios utilizados en esta solución. Adicionalmente, para el ambiente local, se considera diferentes configuraciones utilizando Git, Github, Gitflow y Visual Studio Code. Como requisitos generales, se utiliza: Python 3.9.6 en una MacBook Pro (M1 Pro, 16 Gb RAM, Sonoma).\n",
    "\n",
    "##### 2.3. **feature/bigquery_connector:** \n",
    "Considerando la carga manual realizada y el desarrollo realizado en el punto 2.1. se inicia desarrollando un conector a BigQuery que permita ejecutar las consultas ya desarrolladas en la exploración inicial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Descarga del archivo, descompresión y cargar en Cloud Storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Testing básico de las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "from q1_memory import q1_memory\n",
    "from q1_time import q1_time\n",
    "from q2_memory import q2_memory\n",
    "from q2_time import q2_time\n",
    "from q3_memory import q3_memory\n",
    "from q3_time import q3_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path = \"farmers-protest-tweets-2021-2-4.json\"\n",
    "file_path = \"processed_tweets.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    13     77.5 MiB     77.5 MiB           1   @profile\n",
      "    14                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    15                                             \"\"\"\n",
      "    16                                             Executes a pipeline to load data from Cloud Storage to BigQuery and perform a query.\n",
      "    17                                         \n",
      "    18                                             Args:\n",
      "    19                                                 file_path (str): The path of the file in Cloud Storage.\n",
      "    20                                         \n",
      "    21                                             Returns:\n",
      "    22                                                 List[Tuple[datetime.date, str]]: A list of tuples containing date and string values.\n",
      "    23                                         \n",
      "    24                                             Raises:\n",
      "    25                                                 Exception: Raised if the data pipeline fails.\n",
      "    26                                             \"\"\"\n",
      "    27                                             # Extract table_id from file_path (assuming the file_path is something like \"table_name.json\")\n",
      "    28     77.5 MiB      0.0 MiB           1       table_id = file_path.split(\".\")[0]\n",
      "    29                                         \n",
      "    30     77.5 MiB      0.0 MiB           1       try:\n",
      "    31                                                 # Load data from Cloud Storage to BigQuery\n",
      "    32     77.8 MiB      0.3 MiB           1           success = cloud_storage_to_bigquery(file_path, CS_BUCKET_NAME, BQ_DATASET_ID, table_id)\n",
      "    33                                                 \n",
      "    34     77.8 MiB      0.0 MiB           1           if success:\n",
      "    35                                                     # Process result by querying BigQuery\n",
      "    36     77.8 MiB      0.0 MiB           1               result = read_from_bigquery(BQ_DATASET_ID, table_id, q1)\n",
      "    37     77.8 MiB      0.0 MiB           1               return result\n",
      "    38                                                 else:\n",
      "    39                                                     # If cloud_storage_to_bigquery fails, raise an exception with function name\n",
      "    40                                                     raise Exception(f\"Failed in function 'q1_memory': Cloud Storage to BigQuery pipeline failed.\")\n",
      "    41                                             \n",
      "    42                                             except Exception as e:\n",
      "    43                                                 # Re-raise the exception with the function name included in the error message\n",
      "    44                                                 raise Exception(f\"Error in function 'q1_memory': {str(e)}\")\n",
      "\n",
      "\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "print(q1_memory(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "print(q1_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "     1                                         from typing import List, Tuple  # Importing List and Tuple types for type hints\n",
      "     2                                         from datetime import datetime    # Importing datetime class for date/time operations\n",
      "     3                                         from memory_profiler import profile  # Importing memory_profiler's profile decorator for memory profiling\n",
      "     4                                         from pipelines.queries import q2  # Importing specific query function from pipelines module\n",
      "     5                                         from pipelines.cstorage_and_bq import read_from_bigquery, cloud_storage_to_bigquery  # Importing functions for Cloud Storage and BigQuery operations\n",
      "     6                                         import os  # Importing os module for interacting with the operating system\n",
      "     7     78.2 MiB     78.2 MiB           1   \n",
      "     8                                         \n",
      "     9     78.2 MiB      0.0 MiB           1   # Read environment variables\n",
      "    10     79.6 MiB      1.4 MiB           1   CS_BUCKET_NAME = os.getenv(\"CS_BUCKET_NAME\")\n",
      "    11     79.6 MiB      0.0 MiB          13   BQ_DATASET_ID = os.getenv(\"BQ_DATASET_ID\")\n",
      "    12     79.6 MiB      0.0 MiB           1   \n",
      "    13                                         @profile\n",
      "    14                                         def q2_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    15                                             \"\"\"\n",
      "    16                                             Executes a pipeline to load data from Cloud Storage to BigQuery and perform a query.\n",
      "    17                                         \n",
      "    18                                             Args:\n",
      "    19                                                 file_path (str): The path of the file in Cloud Storage.\n",
      "    20                                         \n",
      "    21                                             Returns:\n",
      "    22                                                 List[Tuple[datetime.date, str]]: A list of tuples containing date and string values.\n",
      "    23                                         \n",
      "    24                                             Raises:\n",
      "    25                                                 Exception: Raised if the data pipeline fails.\n",
      "    26                                             \"\"\"\n",
      "    27                                             # Extract table_id from file_path (assuming the file_path is something like \"table_name.json\")\n",
      "    28                                             table_id = file_path.split(\".\")[0]\n",
      "    29                                         \n",
      "    30                                             try:\n",
      "    31                                                 # Load data from Cloud Storage to BigQuery\n",
      "    32                                                 success = cloud_storage_to_bigquery(file_path, CS_BUCKET_NAME, BQ_DATASET_ID, table_id)\n",
      "    33                                                 \n",
      "    34                                                 if success:\n",
      "    35                                                     # Process result by querying BigQuery\n",
      "    36                                                     result = read_from_bigquery(BQ_DATASET_ID, table_id, q2)\n",
      "    37                                                     return result\n",
      "    38                                                 else:\n",
      "    39                                                     # If cloud_storage_to_bigquery fails, raise an exception with function name\n",
      "    40                                                     raise Exception(f\"Failed in function 'q1_memory': Cloud Storage to BigQuery pipeline failed.\")\n",
      "    41                                             \n",
      "    42                                             except Exception as e:\n",
      "    43                                                 # Re-raise the exception with the function name included in the error message\n",
      "    44                                                 raise Exception(f\"Error in function 'q1_memory': {str(e)}\")\n",
      "\n",
      "\n",
      "[('✊', 2402), ('❤️', 1382), ('❤', 397), ('☮️', 316), ('♂️', 179), ('✌️', 168), ('♀️', 148), ('✌', 106), ('‼️', 74), ('♥️', 73)]\n"
     ]
    }
   ],
   "source": [
    "print(q2_memory(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('✊', 2402), ('❤️', 1382), ('❤', 397), ('☮️', 316), ('♂️', 179), ('✌️', 168), ('♀️', 148), ('✌', 106), ('‼️', 74), ('♥️', 73)]\n"
     ]
    }
   ],
   "source": [
    "print(q2_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    12     79.7 MiB     79.7 MiB           1   @profile\n",
      "    13                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "    14                                             \"\"\"\n",
      "    15                                             Executes a pipeline to load data from Cloud Storage to BigQuery and perform a query.\n",
      "    16                                         \n",
      "    17                                             Args:\n",
      "    18                                                 file_path (str): The path of the file in Cloud Storage.\n",
      "    19                                         \n",
      "    20                                             Returns:\n",
      "    21                                                 List[Tuple[datetime.date, str]]: A list of tuples containing date and string values.\n",
      "    22                                         \n",
      "    23                                             Raises:\n",
      "    24                                                 Exception: Raised if the data pipeline fails.\n",
      "    25                                             \"\"\"\n",
      "    26                                             # Extract table_id from file_path (assuming the file_path is something like \"table_name.json\")\n",
      "    27     79.7 MiB      0.0 MiB           1       table_id = file_path.split(\".\")[0]\n",
      "    28                                         \n",
      "    29     79.7 MiB      0.0 MiB           1       try:\n",
      "    30                                                 # Load data from Cloud Storage to BigQuery\n",
      "    31     79.7 MiB      0.0 MiB           1           success = cloud_storage_to_bigquery(file_path, CS_BUCKET_NAME, BQ_DATASET_ID, table_id)\n",
      "    32                                                 \n",
      "    33     79.7 MiB      0.0 MiB           1           if success:\n",
      "    34                                                     # Process result by querying BigQuery\n",
      "    35     79.7 MiB      0.0 MiB           1               result = read_from_bigquery(BQ_DATASET_ID, table_id, q3)\n",
      "    36     79.7 MiB      0.0 MiB           1               return result\n",
      "    37                                                 else:\n",
      "    38                                                     # If cloud_storage_to_bigquery fails, raise an exception with function name\n",
      "    39                                                     raise Exception(f\"Failed in function 'q3_memory': Cloud Storage to BigQuery pipeline failed.\")\n",
      "    40                                             \n",
      "    41                                             except Exception as e:\n",
      "    42                                                 # Re-raise the exception with the function name included in the error message\n",
      "    43                                                 raise Exception(f\"Error in function 'q3_memory': {str(e)}\")\n",
      "\n",
      "\n",
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "print(q3_memory(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "print(q3_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  2 17:56:47 2024    tmp_time_func_stats\n",
      "\n",
      "         74245 function calls (74117 primitive calls) in 9.647 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 746 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    9.647    9.647 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    9.647    9.647 <string>:1(<module>)\n",
      "        1    0.000    0.000    9.647    9.647 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/q1_time.py:12(q1_time)\n",
      "     10/9    0.000    0.000    9.642    1.071 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:286(retry_wrapped_func)\n",
      "     10/9    0.000    0.000    9.642    1.071 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:85(retry_target)\n",
      "        1    0.000    0.000    8.074    8.074 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/pipelines/cstorage_and_bq.py:10(cloud_storage_to_bigquery)\n",
      "        1    0.000    0.000    7.356    7.356 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/db/bigquery.py:57(load_data_from_uri)\n",
      "        1    0.000    0.000    6.937    6.937 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/cloud/bigquery/job/base.py:941(result)\n",
      "        1    0.000    0.000    6.937    6.937 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/future/polling.py:144(result)\n",
      "        2    0.000    0.000    6.937    3.468 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/future/polling.py:126(_blocking_poll)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x11ad1c610>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cProfile.run('q1_time(file_path)', 'tmp_time_func_stats')\n",
    "p = pstats.Stats(\"tmp_time_func_stats\")\n",
    "p.sort_stats(\"cumulative\").print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  2 17:57:20 2024    ./logs/tmp_time_func_stats\n",
      "\n",
      "         74641 function calls (74511 primitive calls) in 10.175 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 801 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   10.175   10.175 {built-in method builtins.exec}\n",
      "        1    0.000    0.000   10.175   10.175 <string>:1(<module>)\n",
      "        1    0.000    0.000   10.175   10.175 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/q2_time.py:12(q2_time)\n",
      "     10/9    0.000    0.000   10.170    1.130 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:286(retry_wrapped_func)\n",
      "     10/9    0.000    0.000   10.170    1.130 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:85(retry_target)\n",
      "        1    0.000    0.000    8.881    8.881 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/pipelines/cstorage_and_bq.py:10(cloud_storage_to_bigquery)\n",
      "        1    0.000    0.000    7.882    7.882 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/db/bigquery.py:57(load_data_from_uri)\n",
      "        1    0.000    0.000    7.545    7.545 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/cloud/bigquery/job/base.py:941(result)\n",
      "        1    0.000    0.000    7.545    7.545 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/future/polling.py:144(result)\n",
      "        2    0.000    0.000    7.545    3.773 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/future/polling.py:126(_blocking_poll)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x11adb91f0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cProfile.run('q2_time(file_path)', './logs/tmp_time_func_stats')\n",
    "p = pstats.Stats(\"./logs/tmp_time_func_stats\")\n",
    "p.sort_stats(\"cumulative\").print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  2 17:57:32 2024    tmp_time_func_stats\n",
      "\n",
      "         74063 function calls (73935 primitive calls) in 8.534 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 733 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    8.534    8.534 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    8.534    8.534 <string>:1(<module>)\n",
      "        1    0.000    0.000    8.534    8.534 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/q3_time.py:11(q3_time)\n",
      "     10/9    0.000    0.000    8.530    0.948 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:286(retry_wrapped_func)\n",
      "     10/9    0.000    0.000    8.529    0.948 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:85(retry_target)\n",
      "        1    0.000    0.000    7.475    7.475 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/pipelines/cstorage_and_bq.py:10(cloud_storage_to_bigquery)\n",
      "        1    0.000    0.000    6.779    6.779 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/db/bigquery.py:57(load_data_from_uri)\n",
      "        1    0.000    0.000    6.388    6.388 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/cloud/bigquery/job/base.py:941(result)\n",
      "        1    0.000    0.000    6.388    6.388 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/future/polling.py:144(result)\n",
      "        2    0.000    0.000    6.388    3.194 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/future/polling.py:126(_blocking_poll)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x11ac492e0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cProfile.run('q3_time(file_path)', 'tmp_time_func_stats')\n",
    "p = pstats.Stats(\"tmp_time_func_stats\")\n",
    "p.sort_stats(\"cumulative\").print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
