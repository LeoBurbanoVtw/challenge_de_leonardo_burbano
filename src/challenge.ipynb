{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autor:** Leonardo Burbano\n",
    "<br/>\n",
    "**Empresa:** TW\n",
    "<br/>\n",
    "**Email:** leonardo.burbano@[dominioTW]\n",
    "<br/>\n",
    "**Mes/Año:** 05/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Suposiciones\n",
    "\n",
    "- La descarga/carga del archivo de input no se realizará de forma recurrente. Se realizará una sola vez para efectos de solucionar el presente desafío.\n",
    "- La ejecución de la descarga/descompresión/carga del archivo se realizará bajo demanda (y manualmente de ser el caso). Se considerará como variable el nombre del archivo, y como ubicación fija un bucket (repositorio) en Cloud Storage (GCP - Google Cloud Platform).\n",
    "- La prioridad es obtener la respuesta a las preguntas planteadas. No surgirán otras preguntas en el corto plazo.\n",
    "- Al final de este documento se detallan las recomendaciones para garantizar la escalabilidad del proceso y también la calendarización del mismo de ser necesario.\n",
    "- La respuesta proviene de una consulta a la API de Twitter que está almacenada en un archivo comprimido en Google Drive.\n",
    "- El desarrollo no incluye versionamiento de la infraestructura creada o utilización de infraestructura como código.\n",
    "- No se incluyen configuraciones de permisos y otras tareas de disposición de la infraestructura en GCP.\n",
    "- El archivo .zip solo contiene un archivo\n",
    "\n",
    "#### 2. Configuración del ambiente\n",
    "- Se utilizó Git y Gitflow para Mac, para facilitar el versionamiento y simulación de trabajo colaborativo.\n",
    "- Se realizó una exploración inicial en la que se fue\n",
    "- Se utilizará la nube de GCP, se requieré únicamente un archivo de cuenta de servicio con los permisos adecuados para ejecutar el proyecto.\n",
    "\n",
    "\n",
    "#### 2. Estrategia y features a implementar\n",
    "\n",
    "##### 2.1. **feature/initial_exploration:** \n",
    "Se comienza tratando de llegar a la respuesta a las preguntas lo más rápido posible utilizando un Notebook de exploración y las configuraciones manuales en GCP (ejemplo: Cargar un archivo a Cloud Storage, cargar el archivo a BigQuery). Este tipo de pruebas dieron mayor visibilidad del esquema del archivo y los posibles desafíos a enfrentar. El detalle se encuentra en ../_initial_exploration/00_test.ipynb . A partir de esa exploración se realizó esta estrategia y varias de las decisiones tomadas a continuación.\n",
    "\n",
    "##### 2.2 **feature/environment_setup:** \n",
    "Se realiza la configuración del ambiente en GCP creando una \"Service Account\" en la sección de \"IAM\" con los permisos necesarios para cargar datos en BigQuery, Cloud Storage y otros servicios utilizados en esta solución. Adicionalmente, para el ambiente local, se considera diferentes configuraciones utilizando Git, Github, Gitflow y Visual Studio Code. Como requisitos generales, se utiliza: Python 3.9.6 en una MacBook Pro (M1 Pro, 16 Gb RAM, Sonoma).\n",
    "\n",
    "##### 2.3. **feature/bigquery_connector:** \n",
    "Considerando la carga manual realizada y el desarrollo realizado en el punto 2.1. se inicia desarrollando un conector a BigQuery que permita ejecutar las consultas ya desarrolladas en la exploración inicial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Descarga del archivo, descompresión y cargar en Cloud Storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis\n",
      "From (redirected): https://drive.google.com/uc?id=1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis&confirm=t&uuid=919ecb74-4627-4870-9ce9-847f7f817382\n",
      "To: /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/tmp/downloaded_file.zip\n",
      "100%|██████████| 60.4M/60.4M [00:06<00:00, 8.78MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded file saved to: ./tmp/downloaded_file.zip\n",
      "Processed data has been written to './output/farmers-protest-tweets-2021-2-4.json'\n",
      "File uploaded to GCS. URI: gs://de_challenge_leonardo_burbano/farmers-protest-tweets-2021-2-4.json\n",
      "Execution time: 19.66 seconds\n"
     ]
    }
   ],
   "source": [
    "from pipelines.gdrive_and_cstorage import gdrive_to_cstorage #Module to download \n",
    "from db.googledrive import GoogleDriveClient\n",
    "import time\n",
    "\n",
    "# Google Drive file URL (make sure it's the direct download link)\n",
    "url = 'https://drive.google.com/uc?id=1ig2ngoXFTxP5Pa8muXo02mDTFexZzsis'\n",
    "\n",
    "# Record the start time of the execution\n",
    "start_time = time.time()\n",
    "\n",
    "# Call the gdrive_to_cstorage function to transfer the downloaded file from local storage to a cloud storage\n",
    "# Provide the source path of the downloaded file and the destination path in the cloud storage\n",
    "gdrive_to_cstorage(url, \"./tmp/downloaded_file.zip\", \"./input\", \"./output\")\n",
    "\n",
    "# Record the end time of the execution\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total execution time by subtracting start time from end time\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "# Print the execution time with two decimal places\n",
    "print(f\"Execution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Testing básico de las funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"farmers-protest-tweets-2021-2-4.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "from q1_memory import q1_memory\n",
    "from q1_time import q1_time\n",
    "from q2_memory import q2_memory\n",
    "from q2_time import q2_time\n",
    "from q3_memory import q3_memory\n",
    "from q3_time import q3_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/q1_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    13    156.2 MiB    156.2 MiB           1   @profile\n",
      "    14                                         def q1_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    15                                             \"\"\"\n",
      "    16                                             Executes a pipeline to load data from Cloud Storage to BigQuery and perform a query.\n",
      "    17                                         \n",
      "    18                                             Args:\n",
      "    19                                                 file_path (str): The path of the file in Cloud Storage.\n",
      "    20                                         \n",
      "    21                                             Returns:\n",
      "    22                                                 List[Tuple[datetime.date, str]]: A list of tuples containing date and string values.\n",
      "    23                                         \n",
      "    24                                             Raises:\n",
      "    25                                                 Exception: Raised if the data pipeline fails.\n",
      "    26                                             \"\"\"\n",
      "    27                                             # Extract table_id from file_path (assuming the file_path is something like \"table_name.json\")\n",
      "    28    156.2 MiB      0.0 MiB           1       table_id = file_path.split(\".\")[0]\n",
      "    29                                         \n",
      "    30    156.2 MiB      0.0 MiB           1       try:\n",
      "    31                                                 # Load data from Cloud Storage to BigQuery\n",
      "    32    156.6 MiB      0.4 MiB           1           success = cloud_storage_to_bigquery(file_path, CS_BUCKET_NAME, BQ_DATASET_ID, table_id)\n",
      "    33                                                 \n",
      "    34    156.6 MiB      0.0 MiB           1           if success:\n",
      "    35                                                     # Process result by querying BigQuery\n",
      "    36    156.6 MiB      0.0 MiB           1               result = read_from_bigquery(BQ_DATASET_ID, table_id, q1)\n",
      "    37    156.6 MiB      0.0 MiB           1               return result\n",
      "    38                                                 else:\n",
      "    39                                                     # If cloud_storage_to_bigquery fails, raise an exception with function name\n",
      "    40                                                     raise Exception(f\"Failed in function 'q1_memory': Cloud Storage to BigQuery pipeline failed.\")\n",
      "    41                                             \n",
      "    42                                             except Exception as e:\n",
      "    43                                                 # Re-raise the exception with the function name included in the error message\n",
      "    44                                                 raise Exception(f\"Error in function 'q1_memory': {str(e)}\")\n",
      "\n",
      "\n",
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "print(q1_memory(file_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "print(q1_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/q2_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    13    156.6 MiB    156.6 MiB           1   @profile\n",
      "    14                                         def q2_memory(file_path: str) -> List[Tuple[datetime.date, str]]:\n",
      "    15                                             \"\"\"\n",
      "    16                                             Executes a pipeline to load data from Cloud Storage to BigQuery and perform a query.\n",
      "    17                                         \n",
      "    18                                             Args:\n",
      "    19                                                 file_path (str): The path of the file in Cloud Storage.\n",
      "    20                                         \n",
      "    21                                             Returns:\n",
      "    22                                                 List[Tuple[datetime.date, str]]: A list of tuples containing date and string values.\n",
      "    23                                         \n",
      "    24                                             Raises:\n",
      "    25                                                 Exception: Raised if the data pipeline fails.\n",
      "    26                                             \"\"\"\n",
      "    27                                             # Extract table_id from file_path (assuming the file_path is something like \"table_name.json\")\n",
      "    28    156.6 MiB      0.0 MiB           1       table_id = file_path.split(\".\")[0]\n",
      "    29                                         \n",
      "    30    156.6 MiB      0.0 MiB           1       try:\n",
      "    31                                                 # Load data from Cloud Storage to BigQuery\n",
      "    32    156.6 MiB      0.0 MiB           1           success = cloud_storage_to_bigquery(file_path, CS_BUCKET_NAME, BQ_DATASET_ID, table_id)\n",
      "    33                                                 \n",
      "    34    156.6 MiB      0.0 MiB           1           if success:\n",
      "    35                                                     # Process result by querying BigQuery\n",
      "    36    156.7 MiB      0.0 MiB           1               result = read_from_bigquery(BQ_DATASET_ID, table_id, q2)\n",
      "    37    156.7 MiB      0.0 MiB           1               return result\n",
      "    38                                                 else:\n",
      "    39                                                     # If cloud_storage_to_bigquery fails, raise an exception with function name\n",
      "    40                                                     raise Exception(f\"Failed in function 'q1_memory': Cloud Storage to BigQuery pipeline failed.\")\n",
      "    41                                             \n",
      "    42                                             except Exception as e:\n",
      "    43                                                 # Re-raise the exception with the function name included in the error message\n",
      "    44                                                 raise Exception(f\"Error in function 'q1_memory': {str(e)}\")\n",
      "\n",
      "\n",
      "[('✊', 2402), ('❤️', 1382), ('❤', 397), ('☮️', 316), ('♂️', 179), ('✌️', 168), ('♀️', 148), ('✌', 106), ('‼️', 74), ('♥️', 73)]\n"
     ]
    }
   ],
   "source": [
    "print(q2_memory(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('✊', 2402), ('❤️', 1382), ('❤', 397), ('☮️', 316), ('♂️', 179), ('✌️', 168), ('♀️', 148), ('✌', 106), ('‼️', 74), ('♥️', 73)]\n"
     ]
    }
   ],
   "source": [
    "print(q2_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/q3_memory.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    12    156.7 MiB    156.7 MiB           1   @profile\n",
      "    13                                         def q3_memory(file_path: str) -> List[Tuple[str, int]]:\n",
      "    14                                             \"\"\"\n",
      "    15                                             Executes a pipeline to load data from Cloud Storage to BigQuery and perform a query.\n",
      "    16                                         \n",
      "    17                                             Args:\n",
      "    18                                                 file_path (str): The path of the file in Cloud Storage.\n",
      "    19                                         \n",
      "    20                                             Returns:\n",
      "    21                                                 List[Tuple[datetime.date, str]]: A list of tuples containing date and string values.\n",
      "    22                                         \n",
      "    23                                             Raises:\n",
      "    24                                                 Exception: Raised if the data pipeline fails.\n",
      "    25                                             \"\"\"\n",
      "    26                                             # Extract table_id from file_path (assuming the file_path is something like \"table_name.json\")\n",
      "    27    156.7 MiB      0.0 MiB           1       table_id = file_path.split(\".\")[0]\n",
      "    28                                         \n",
      "    29    156.7 MiB      0.0 MiB           1       try:\n",
      "    30                                                 # Load data from Cloud Storage to BigQuery\n",
      "    31    156.7 MiB      0.0 MiB           1           success = cloud_storage_to_bigquery(file_path, CS_BUCKET_NAME, BQ_DATASET_ID, table_id)\n",
      "    32                                                 \n",
      "    33    156.7 MiB      0.0 MiB           1           if success:\n",
      "    34                                                     # Process result by querying BigQuery\n",
      "    35    156.7 MiB      0.0 MiB           1               result = read_from_bigquery(BQ_DATASET_ID, table_id, q3)\n",
      "    36    156.7 MiB      0.0 MiB           1               return result\n",
      "    37                                                 else:\n",
      "    38                                                     # If cloud_storage_to_bigquery fails, raise an exception with function name\n",
      "    39                                                     raise Exception(f\"Failed in function 'q3_memory': Cloud Storage to BigQuery pipeline failed.\")\n",
      "    40                                             \n",
      "    41                                             except Exception as e:\n",
      "    42                                                 # Re-raise the exception with the function name included in the error message\n",
      "    43                                                 raise Exception(f\"Error in function 'q3_memory': {str(e)}\")\n",
      "\n",
      "\n",
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "print(q3_memory(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "print(q3_time(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  2 23:05:11 2024    tmp_time_func_stats\n",
      "\n",
      "         76589 function calls (76461 primitive calls) in 8.656 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 746 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    8.656    8.656 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    8.656    8.656 <string>:1(<module>)\n",
      "        1    0.000    0.000    8.656    8.656 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/q1_time.py:12(q1_time)\n",
      "     10/9    0.000    0.000    8.652    0.961 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:286(retry_wrapped_func)\n",
      "     10/9    0.000    0.000    8.652    0.961 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:85(retry_target)\n",
      "        1    0.000    0.000    6.855    6.855 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/pipelines/cstorage_and_bq.py:10(cloud_storage_to_bigquery)\n",
      "        1    0.000    0.000    6.163    6.163 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/db/bigquery.py:56(load_data_from_uri)\n",
      "        1    0.000    0.000    5.766    5.766 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/cloud/bigquery/job/base.py:941(result)\n",
      "        1    0.000    0.000    5.766    5.766 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/future/polling.py:144(result)\n",
      "        2    0.000    0.000    5.766    2.883 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/future/polling.py:126(_blocking_poll)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x13047e070>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cProfile.run('q1_time(file_path)', 'tmp_time_func_stats')\n",
    "p = pstats.Stats(\"tmp_time_func_stats\")\n",
    "p.sort_stats(\"cumulative\").print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  2 23:05:24 2024    ./logs/tmp_time_func_stats\n",
      "\n",
      "         88739 function calls (88605 primitive calls) in 12.171 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 733 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000   12.171   12.171 {built-in method builtins.exec}\n",
      "        1    0.000    0.000   12.171   12.171 <string>:1(<module>)\n",
      "        1    0.000    0.000   12.171   12.171 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/q2_time.py:12(q2_time)\n",
      "     10/9    0.000    0.000   12.166    1.352 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:286(retry_wrapped_func)\n",
      "     10/9    0.001    0.000   12.166    1.352 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:85(retry_target)\n",
      "        1    0.000    0.000   10.716   10.716 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/pipelines/cstorage_and_bq.py:10(cloud_storage_to_bigquery)\n",
      "        1    0.000    0.000   10.058   10.058 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/db/bigquery.py:56(load_data_from_uri)\n",
      "        1    0.000    0.000    9.583    9.583 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/cloud/bigquery/job/base.py:941(result)\n",
      "        1    0.000    0.000    9.583    9.583 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/future/polling.py:144(result)\n",
      "        2    0.000    0.000    9.583    4.791 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/future/polling.py:126(_blocking_poll)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x1304dc370>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cProfile.run('q2_time(file_path)', './logs/tmp_time_func_stats')\n",
    "p = pstats.Stats(\"./logs/tmp_time_func_stats\")\n",
    "p.sort_stats(\"cumulative\").print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu May  2 23:05:33 2024    tmp_time_func_stats\n",
      "\n",
      "         76934 function calls (76806 primitive calls) in 8.398 seconds\n",
      "\n",
      "   Ordered by: cumulative time\n",
      "   List reduced from 734 to 10 due to restriction <10>\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000    8.398    8.398 {built-in method builtins.exec}\n",
      "        1    0.000    0.000    8.398    8.398 <string>:1(<module>)\n",
      "        1    0.000    0.000    8.398    8.398 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/q3_time.py:11(q3_time)\n",
      "     10/9    0.000    0.000    8.395    0.933 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:286(retry_wrapped_func)\n",
      "     10/9    0.000    0.000    8.395    0.933 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:85(retry_target)\n",
      "        1    0.000    0.000    7.252    7.252 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/pipelines/cstorage_and_bq.py:10(cloud_storage_to_bigquery)\n",
      "        1    0.000    0.000    6.545    6.545 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/src/db/bigquery.py:56(load_data_from_uri)\n",
      "        1    0.000    0.000    6.123    6.123 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/cloud/bigquery/job/base.py:941(result)\n",
      "        1    0.000    0.000    6.123    6.123 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/future/polling.py:144(result)\n",
      "        2    0.000    0.000    6.123    3.061 /Users/leonardoburbano/LatamAir/challenge_de_leonardo_burbano/venv_dev/lib/python3.9/site-packages/google/api_core/future/polling.py:126(_blocking_poll)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pstats.Stats at 0x1304dc3a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cProfile.run('q3_time(file_path)', 'tmp_time_func_stats')\n",
    "p = pstats.Stats(\"tmp_time_func_stats\")\n",
    "p.sort_stats(\"cumulative\").print_stats(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
